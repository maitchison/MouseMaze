{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Tree $\\Phi$MDP\n",
    "\n",
    "As per https://arxiv.org/pdf/1108.3614.pdf\n",
    "\n",
    "I'm not sure how the Markovian property for context trees work?  In the paper a Markovian AOCT is given on page 12.  But if the history is ?0 -> 0 then action observation (21) is performed we end up on one of 3 states.  I'm therefore not sure what the definition of Markovian is for context trees.\n",
    "\n",
    "Also the splitting / merging algorithm is quite complex.  A simpler algorithm might be to just mutate and check if it's valid, or... maybe even to just allow non MDP trees..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kruskal\n",
    "import gym\n",
    "\n",
    "from maze import Maze_MDP, Maze_POMDP\n",
    "from utilities import clip, smooth, int_to_bits, bits_to_int, softmax\n",
    "from collections import deque\n",
    "from gym import error, spaces, utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for algorithm, taken from Nguyen's paper\n",
    "\n",
    "ALPHA = 0.1\n",
    "BETA = 0.1\n",
    "INITIAL_SAMPLE_LENGTH = 5000\n",
    "AGENT_LEARNING_LOOPS = 1\n",
    "ITERATIONS = 100\n",
    "I = 10\n",
    "GAMMA = 0.999999 # ??? why is this so high?\n",
    "ETA = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a maze for us to solve.\n",
    "maze = Maze_POMDP(7,7)\n",
    "maze.generate_random(4)\n",
    "maze.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Map Cost Function\n",
    "\n",
    "We define a cost function for a given map based on how well it can predict future rewards as follows:\n",
    "\n",
    "$$\n",
    "Cost_{\\alpha}(\\Phi|h_n) := \\alpha CL(s_{1:n}|a_{1:n})+(1-\\alpha)CL(r_{1:n}|s_{1:n},a_{1:n})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\" Shannon entropy of a random variable with distribution p\"\"\"\n",
    "    return np.sum(p * np.log2(p))\n",
    "\n",
    "def code_length(history):\n",
    "    \"\"\" returns code length of states given actions for given map.\n",
    "        @param history Array of (state, action, reward) for every timestep.\n",
    "    \"\"\"\n",
    "        \n",
    "    States = set(s for s,a,r in history)\n",
    "    Actions = set(a for s,a,r in history)\n",
    "    Rewards = set(r for s,a,r in history)\n",
    "    \n",
    "    # make a history that is in the format s,a,r,s' where s' is the resulting state.\n",
    "    _history = history\n",
    "    history = []\n",
    "    for i in range(len(_history)-1):        \n",
    "        history.append((*_history[i], _history[i+1]))\n",
    "    \n",
    "    # code length for states given actions based on estimated frequencies\n",
    "    state_code_length = 0\n",
    "    for action in Actions:                    \n",
    "        for state in States:\n",
    "            state_action_counts = np.zeros(len(States))\n",
    "            for s,a,r,s_prime in history:\n",
    "                if s != state or a != action:\n",
    "                    continue\n",
    "                state_action_counts[s_prime] += 1\n",
    "            state_action_sum = np.sum(state_action_counts)    \n",
    "                \n",
    "            state_code_length += state_action_sum * entropy(state_action_counts / state_action_sum)\n",
    "            state_code_length += (len(States) - 1) / 2 * np.log2(state_action_sum)\n",
    "            \n",
    "    # code length for rewards given state and actions.\n",
    "    reward_code_length\n",
    "    for action in Actions:                    \n",
    "        for state in States:\n",
    "            for state_prime in States:\n",
    "                state_reward_counts = np.zeros(len(Rewards))\n",
    "                for s,a,r,s_prime in history:\n",
    "                    if s != state or a != action or s_prime != state_prime:\n",
    "                        continue\n",
    "                    state_reward_counts[r] += 1\n",
    "                state_reward_sum = np.sum(state_reward_counts)    \n",
    "            \n",
    "                reward_code_length = state_reward_sum * entropy(state_reward_counts / state_reward_sum)\n",
    "                reward_code_length += (len(Rewards) - 1) / 2 * np.log2(state_reward_sum)\n",
    "\n",
    "    return ALPHA * state_code_length + (1-ALPHA)*reward_code_length\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Stochastic Search Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function.  Evaluate how good a map is given history.\n",
    "# This should essentially be how well the MDP generated predicts rewards.\n",
    "\n",
    "def map_cost(phi, history):\n",
    "    \"\"\" Returns cost of using map phi, with supplied history. \n",
    "        @param phi a map...\n",
    "        @param history Array of observational history (observation,action,reward)\n",
    "    \"\"\"\n",
    "    history_with_states = [(phi(history[:n]), a, r) for n, (o,a,r) in enumerate(history)]\n",
    "    return code_length(history_with_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random Search\n",
    "\n",
    "This is just a very simple search algoritm to get us up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-0', '0-1', '1-0', '1-1']\n",
      "['0', '1-0-0', '1-0-1', '1-1']\n",
      "1-0-1\n"
     ]
    }
   ],
   "source": [
    "class Node():\n",
    "    def __init__(self, children):\n",
    "        \"\"\" \n",
    "            param: children dictionary from edge to node\n",
    "        \"\"\"\n",
    "        self.children = children                    \n",
    "        \n",
    "    def get_states(self):\n",
    "        \"\"\" Returns list of all states this node  enables. \"\"\"        \n",
    "        result = []\n",
    "        for edge, child in self.children.items():\n",
    "            if child is None:\n",
    "                result += [str(edge)]\n",
    "            else:                    \n",
    "                sub_states = child.get_states()  \n",
    "                result += [str(edge)+'-'+state for state in sub_states]\n",
    "        return result                    \n",
    "    \n",
    "\n",
    "class ContextTree():\n",
    "        \n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "            \n",
    "    def encode(self, history):\n",
    "        \"\"\" Maps from list of (observation) to state space. \n",
    "            @param history, (observation)\n",
    "        \"\"\"\n",
    "        current = self.root\n",
    "        \n",
    "        states = []\n",
    "\n",
    "        for o in reversed(history):\n",
    "            if current is not None:\n",
    "                current = current.children[o]\n",
    "                states.append(o)\n",
    "            else: \n",
    "                break\n",
    "                \n",
    "        return \"-\".join(str(x) for x in states)\n",
    "    \n",
    "    def is_markovian(self, Observations):\n",
    "        \"\"\" Checks if this tree is Markovian or not.\"\"\"\n",
    "        for state in self.root.get_states():\n",
    "            for observation in Observations:\n",
    "                new_state \n",
    "        \n",
    "class ActionContextTree():\n",
    "    \n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "    \n",
    "    def encode(self, history):\n",
    "        \"\"\" Maps from list of (observation) to state space. \n",
    "            @param history, (observation)\n",
    "        \"\"\"\n",
    "        current = self.root\n",
    "        \n",
    "        states = []\n",
    "\n",
    "        for (o,a,r) in reversed(history):\n",
    "            if current is not None:\n",
    "                current = current[o]\n",
    "                states.append(o)\n",
    "            else: \n",
    "                break\n",
    "            if current is not None:\n",
    "                current = current[a]\n",
    "                states.append(a)\n",
    "            else: \n",
    "                break\n",
    "                                \n",
    "        return \"-\".join(str(x) for x in states)\n",
    "    \n",
    "    \n",
    "# these are the tests from Nyugen's paper\n",
    "\n",
    "Left = Node({0:None, 1:None})\n",
    "Right = Node({0:None, 1:None})\n",
    "ct = ContextTree(Node({0:Left, 1:Right}))\n",
    "\n",
    "print(ct.root.get_states())\n",
    "\n",
    "ct = ContextTree(Node({0:None, 1:None}))\n",
    "ct.root.children[1] = Node({0:None, 1:None})\n",
    "ct.root.children[1].children[0] = Node({0:None, 1:None})\n",
    "\n",
    "print(ct.root.get_states())\n",
    "print(ct.encode([int(x) for x in \"11101\"]))\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(h_initial, cost_function, iterations=1000):\n",
    "    \"\"\" Peform a random search for a low cost map.\"\"\"\n",
    "    best_cost = float(\"inf\")\n",
    "    best_phi = None\n",
    "    for i in range(iterations):\n",
    "        # construct a random map\n",
    "        phi = ...\n",
    "        # evaluate it\n",
    "        cost = cost_function(phi, h_initial)\n",
    "        if cost < best_cost:\n",
    "            best_phi = phi\n",
    "            best_cost\n",
    "    return phi\n",
    "            \n",
    "def random_context_tree(Observation, Action):\n",
    "    \"\"\" Not a very good random context tree generator... but will probably do for testing.\n",
    "        @param Observations observation space\n",
    "        @param Action action space        \n",
    "    \"\"\"\n",
    "    \n",
    "    root = np.random.choose(Observation)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parallel Temporing Algorithm\n",
    "\n",
    "Perform parallel storechastic search over maps $\\phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  todo.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. $\\Phi$ MDP Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Generate a history h^initial of length initialSampleNumber\n",
    "\n",
    "obs = maze.reset()\n",
    "h_initial = []\n",
    "for i in range(INITIAL_SAMPLE_LENGTH):\n",
    "    action = np.random.randint(len(maze.ACTIONS))\n",
    "    obs, reward, done, info = maze.step(action)\n",
    "    h_initial.append((obs, reward, action))\n",
    "    \n",
    "# 2: h←h^initial\n",
    "h = h_initial.copy()\n",
    "\n",
    "# 3: repeat\n",
    "while True:\n",
    "    \n",
    "    #4: Run the chosen stochastic search scheme for the history h to find a Φˆ with low cost\n",
    "    random_search(h_initial)\n",
    "    \n",
    "    #5: Compute MDP statistics (optimistic frequency estimates Rˆ and Tˆ) induced from Φˆ\n",
    "    #6: Apply AVI to find the optimal Q^* values using the computed statistics Rˆ and Tˆ.\n",
    "    #7: Interact with environment for additionalSampleNumber iterations of Q-Learning using Q^∗ as initial values; the obtained additional history is stored in h additional\n",
    "    # 8: h←[h,hadditional]\n",
    "    # 9: agentLearningLoops←agentLearningLoops−1\n",
    "    # 10: until agentLearningLoops= 0\n",
    "    # 11: Compute the optimal policy π optimal from the optimal Φ and Q values\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
