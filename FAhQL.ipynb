{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAhQL \n",
    "\n",
    "History based Q-Learning\n",
    "\n",
    "As per http://proceedings.mlr.press/v29/Daswani13.pdf\n",
    "\n",
    "The idea is to extract features form this history, then estimate the Q values from these features directly (i.e. without learning a model)\n",
    "\n",
    "Two methods for extracting features talked about in the paper.\n",
    "\n",
    "* **Context trees** which provide a robust way to learn context dependant history lengths.\n",
    "\n",
    "* **Event selector** which extracts a set of observations from specific times prior in history.\n",
    "\n",
    "Once features have been extracted a linear function approximator is learned by mimizing squared Q-learning error.\n",
    "\n",
    "We implement just the FAhQL algorithm here.\n",
    "\n",
    "$\\mathbf{\\text{Cost}_{\\text{QL}}(\\xi)}$\n",
    "\n",
    "We calculate the cost of a particiular event selector set $\\xi$ as \n",
    "\n",
    "$$Cost_{QL}(\\xi) := \\min_w \\frac{1}{2} \\sum_{t=1}^n (r_{t+1} + \\gamma \\max_a \\xi (h_{t+1},a_t)^T w) ^2 + \\text{reg}(\\xi)$$\n",
    "\n",
    "That is, for the optimal $w$, how well did the function approximator approximate $Q$ values over the trajectory.\n",
    "The regularization used is $\\frac{\\beta}{2}k\\log_2(n)$ which for fix history size is simply the $l0$ norm, i.e. propotional to $k$ the number of features present in the map.\n",
    "\n",
    "**Jumping Distribution**\n",
    "In this case the jumping distribution is simply adding or removing a single feature.\n",
    "\n",
    "**The Agent**\n",
    "\n",
    "We start by giving the agent some initial history generated with random actions, and a starting map.\n",
    "\n",
    "Epoch\n",
    "\n",
    "1. Generate a new map, and either keep our current map or switch to the new one\n",
    "\n",
    "2. Calculate $Q$ values based on states generated by our current map\n",
    "\n",
    "3. Act according to the policy formed by $Q$ for m steps.\n",
    "\n",
    "**Parameters used in paper**\n",
    "\n",
    "100 Epochs\n",
    "Each Epoch has 100 iterations\n",
    "Stop $\\epsilon$ exploration after 50 epochs\n",
    "Run each experiment 30 times.\n",
    "Number of annealing steps was 10 or 20 depending on algorithm / problem\n",
    "An initial history of 200 time steps with random actions was used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from utils.utils import clip, smooth, int_to_bits, bits_to_int, softmax\n",
    "from agents.agent import Agent\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial parameters\n",
    "\n",
    "# todo: initialize these as per paper...\n",
    "\n",
    "BETA = 1.0\n",
    "GAMMA = 0.99\n",
    "\n",
    "# our action space\n",
    "action_space = []\n",
    "\n",
    "# our observation space\n",
    "observation_space = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_annealing(history, initial_map, cost, cooling_schedule, get_neighbour, N=10)\n",
    "    \"\"\" \n",
    "        Find an improved map via simulated annealing.\n",
    "        @param history The history so far, (observation, action, reward)\n",
    "        @param initial_map An initial starting map\n",
    "        @param cost A function mapping from (history, map) to the cost of that map\n",
    "        @param schceule Temperature cooling schedule\n",
    "        @param get_neighbour Function taking a map and generating a random neighbour\n",
    "        @param n Number of simulated annealing iterations        \n",
    "    \"\"\"\n",
    "    current_map = best_map = initial_map\n",
    "    current_cost = best_cost = cost(h, initial_map)\n",
    "    for t in range(N):\n",
    "        candidate_map = get_neighbour(current_map)\n",
    "        candidate_cost = cost(h, candidate_map)\n",
    "        delta = current_cost - candiate_cost\n",
    "        T = cooling_schedule(t)\n",
    "        p = np.random.rand()\n",
    "        if delta > 0 or p < np.exp(delta/T):\n",
    "            current_map = candidate_map\n",
    "            current_cost = candidate_cost\n",
    "            if current_cost < best_cost:\n",
    "                best_map = current_map\n",
    "                best_cost = current_cost    \n",
    "                \n",
    "def get_neighbour(initial_map):\n",
    "    \"\"\" Returns the neighbour of a event selector. \"\"\"\n",
    "    new_map = initial_map.copy()\n",
    "    if np.random.randint(2) == 0:        \n",
    "        \"\"\" Add an event. \"\"\"\n",
    "        random_observation = np.random.randint(len(observation_space))\n",
    "        # the paper doesn't document how to select m, so I'm using poisson\n",
    "        m = np.random.poisson(5)\n",
    "        initial_map.append((m, random_observation))\n",
    "    else:\n",
    "        \"\"\" Remove an event. \"\"\"\n",
    "        del new_map[np.random.randint(len(new_map))]\n",
    "    return new_map                                    \n",
    "\n",
    "def cooling_schedule(t, start_T, cool_rate):\n",
    "    \"\"\" Expoential cooling decay. \"\"\"\n",
    "    return start_T * np.exp(-cool_rate * t)\n",
    "                           \n",
    "def FAQ_learn(initial_w, states, rewards, actions):\n",
    "    \"\"\"\n",
    "        Q learning via function approximation.\n",
    "        This is learned off-policy with the supplied states rewards, and actions.\n",
    "        If necessary the history is looped over.\n",
    "                \n",
    "        @param initial_w The initial weights for function approximation\n",
    "        @param states State history list\n",
    "        @param states Reward history list\n",
    "        @param states Action history list\n",
    "        \n",
    "        @return The optimal weights for linear function approximation\n",
    "    \"\"\"\n",
    "    \n",
    "    #this update was taken from\n",
    "    #https://openresearch-repository.anu.edu.au/bitstream/1885/110545/1/Daswani%20Thesis%202016.pdf\n",
    "    \n",
    "    w = initial_w\n",
    "    \n",
    "    for i in range(1000):\n",
    "        \n",
    "        state   = states[(i+1) % len(states)]\n",
    "        reward  = reward[(i+1) % len(reward)]\n",
    "        action  = action[(i+1) % len(action)]\n",
    "        prev_state  = states[i % len(states)]\n",
    "        prev_reward = reward[i % len(reward)]\n",
    "        prev_action = action[i % len(action)]\n",
    "                                    \n",
    "        # in this case phi(s,a) is just s, but really we should be including a somehow...\n",
    "        delta = reward + GAMMA * (w @ state) - (w @ prev_state)\n",
    "        \n",
    "        # todo add normalizating constant, sum_i phi_i(s,a)        \n",
    "        w = w + alpha * delta * phi(state,action)\n",
    "        \n",
    "        prev_state = state\n",
    "        prev_action = action        \n",
    "    \n",
    "    \n",
    "def cost(phi, history):\n",
    "    \"\"\" Cost of given map based on history. \n",
    "        @param phi map from history to state\n",
    "        @param history list of (observation, action, reward) \n",
    "    \"\"\"\n",
    "    \n",
    "    observations, actions, rewards = zip(*history)\n",
    "    \n",
    "    # generate our states via the mapping phi. \n",
    "    states = []\n",
    "    for t in range(len(history)):\n",
    "        states.append(phi(history[:t]))\n",
    "    \n",
    "    rewards = phi(event for event in history)\n",
    "    \n",
    "    # learn the best weights for our linear function approximation\n",
    "    w = Q_learn(states, rewards, actions)\n",
    "    \n",
    "    # not sure how to include actions in the map phi, so I'll just ignore the action.\n",
    "    # this ends up being a state value rather than a action-state value though?\n",
    "    # really want we want is seperate weights for each action based on the features.\n",
    "    xi = lambda history, action : phi(history)\n",
    "    \n",
    "    return cost_ql(w, xi, history)\n",
    "    \n",
    "\n",
    "def cost_ql(w, xi, history):\n",
    "    \"\"\" Calculate the cost of the feature extractor xi, according to some history.\n",
    "        @param w weights for linear function approximation (np array)\n",
    "        @param xi function from (history,action) to feature vector.\n",
    "        @param history list of (observation, reward, action)        \n",
    "    \"\"\"\n",
    "    \n",
    "    # length of feature vector\n",
    "    k = length(w)\n",
    "    \n",
    "    # length of history\n",
    "    n = length(history)\n",
    "    \n",
    "    regularization = BETA / 2 * k * log_2(n)\n",
    "    \n",
    "    cost_sum = 0\n",
    "    for t in range(1,n):\n",
    "        \n",
    "        observation_prev, reward_prev, action_prev = history[t-1]\n",
    "        observation, reward, action = history[t]\n",
    "        \n",
    "        action_value = [xi(history[:t+1], a) @ w for a in action_space]\n",
    "                    \n",
    "        td = (reward + GAMMA * max(action_value)) - xi(history[:t], action_prev) @ w\n",
    "        \n",
    "        cost_sum += 0.5*(td**2)\n",
    "         \n",
    "    return cost_sum + regularization\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for @: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c00298e98952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for @: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "class FAhQL(Agent):\n",
    "    \"\"\" Function approximation history based Q-Learning agent.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "            \n",
    "    def act(self, observation, reward):\n",
    "        \"\"\" find optimal action based on observation and reward. \"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "    \n",
    "Daswani seems to be finding a map the mimizes the TD error, however according to http://papers.nips.cc/paper/8200-non-delusional-q-learning-and-value-iteration.pdf it is not a good idea to mimize the TD error, but rather the belman error.  Perhaps, in this case, as we are trying to find a good map, rather then solve it, this is still helpful?\n",
    "        \n",
    "**Questions:**\n",
    "What is the difference between hQL and CT-MDP?\n",
    "\n",
    "Why are we using $\\frac{\\beta}{2}k\\log_2(n)$ as the regulizer.  Seems strange that the the cost of $\\xi$ increases with the length of the history?  Shouldn't it be something like $\\log_2(n^*)$ where $n^*$ is the longest time we look back in history?\n",
    "* My thoughts on this is that the number of bits require for each feature selector index increases logarithimically with the length of the history.\n",
    "\n",
    "Are there any guarintees that the event selector will generate a MDP, or do we not care.\n",
    "* Talked to Daswani about this, said it won't be a MDP and refered me to Sultan's 2017 paper.\n",
    "\n",
    "Seems like maybe the suffix tree defined here is much simpler than the context tree described by Nyguen.  No talk about dealing with MDP constrain though?\n",
    "\n",
    "Why is the event selector observation only?  This would make the Maze4x4 problem unsolvable as it gives uninformative observations.\n",
    "\n",
    "Cost algorithm: What is $\\phi_{1:n}$\n",
    "\n",
    "Cost algorithm: Shouldn't $\\phi(h_t)$ be $\\phi(h_{1:t})$?\n",
    "\n",
    "Why is $\\xi$ a map from history and action to a feature vector, not just history?\n",
    "* Have a look at  https://danieltakeshi.github.io/2016/10/31/going-deeper-into-reinforcement-learning-understanding-q-learning-and-linear-function-approximation/, it explains this quite well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
